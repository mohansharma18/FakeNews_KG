{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__)').html -q\n",
        "\n",
        "# Install other necessary libraries\n",
        "!pip install datasets pandas numpy networkx scikit-learn transformers sentence-transformers -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iL7xQS562uE",
        "outputId": "c9cdedee-b2fc-4d06-dc0a-0340c3ada950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx # For initial graph understanding (optional)\n",
        "import time\n",
        "import sys\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "\n",
        "# Machine Learning and NLP\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Embedding, Linear, ModuleList, ReLU\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import SAGEConv, HeteroConv, global_mean_pool, to_hetero\n",
        "from torch_geometric.loader import DataLoader # For potential batching if needed\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer # Potentially useful\n",
        "from sentence_transformers import SentenceTransformer # For statement embeddings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Ignore common warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# --- Configuration ---\n",
        "SAMPLE_SIZE = 1000     # Number of statements to process (adjust based on resources/time)\n",
        "TEST_SPLIT_RATIO = 0.2 # Ratio for train/test split\n",
        "EMBEDDING_DIM = 64    # Dimension for learnable entity embeddings (speaker, subject, etc.)\n",
        "SBERT_MODEL = 'all-MiniLM-L6-v2' # Sentence Transformer model for statement features\n",
        "HIDDEN_CHANNELS = 128  # GNN hidden layer dimension\n",
        "NUM_GNN_LAYERS = 2    # Number of GNN layers\n",
        "NUM_EPOCHS = 50       # Training epochs\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 128      # Batch size for training (if using DataLoader) - Simpler approach without batching first\n",
        "\n",
        "# Map numeric labels to text labels (consistent naming)\n",
        "LABEL_MAP = {\n",
        "    0: 'false', 1: 'half-true', 2: 'mostly-true',\n",
        "    3: 'true', 4: 'barely-true', 5: 'pants-on-fire'\n",
        "}\n",
        "# Inverse map for potential use\n",
        "INV_LABEL_MAP = {v: k for k, v in LABEL_MAP.items()}\n",
        "CANDIDATE_LABELS = list(LABEL_MAP.values())\n",
        "\n",
        "# Define node and edge types\n",
        "NODE_TYPES = ['statement', 'speaker', 'subject', 'party']\n",
        "EDGE_TYPES = [\n",
        "    ('statement', 'spoken_by', 'speaker'),\n",
        "    ('speaker', 'affiliated_with', 'party'),\n",
        "    ('statement', 'about_subject', 'subject'),\n",
        "    # Optional: Inverse edges if needed by the GNN architecture\n",
        "    ('speaker', 'rev_spoken_by', 'statement'),\n",
        "    ('party', 'rev_affiliated_with', 'speaker'),\n",
        "    ('subject', 'rev_about_subject', 'statement'),\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 1. Load and Prepare Data ---\n",
        "# ==============================================================================\n",
        "def load_and_prep_data(sample_size):\n",
        "    \"\"\"Loads LIAR dataset, samples, and performs initial prep.\"\"\"\n",
        "    print(\"--- 1. Loading and Preparing Data ---\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        dataset = load_dataset(\"liar\")\n",
        "        df_full = dataset['train'].to_pandas()\n",
        "        print(f\"Full dataset shape: {df_full.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        sys.exit()\n",
        "\n",
        "    # Sample data\n",
        "    if sample_size < len(df_full):\n",
        "        df_sample = df_full.sample(n=sample_size, random_state=42).copy()\n",
        "        print(f\"Using a sample of {sample_size} rows.\")\n",
        "    else:\n",
        "        df_sample = df_full.copy()\n",
        "        print(f\"Using the full dataset ({len(df_sample)} rows).\")\n",
        "\n",
        "    # Map numeric labels to text and add integer labels\n",
        "    if 'label' in df_sample.columns and pd.api.types.is_numeric_dtype(df_sample['label']):\n",
        "        df_sample['label_text'] = df_sample['label'].map(LABEL_MAP)\n",
        "        # Use original numeric label directly for training targets\n",
        "        df_sample['label_idx'] = df_sample['label']\n",
        "        print(\"Created 'label_text' and 'label_idx' columns.\")\n",
        "    else:\n",
        "        print(\"Warning: Original numeric 'label' column not found or not numeric.\")\n",
        "        # Attempt to create numeric labels if text labels exist\n",
        "        if 'label' in df_sample.columns:\n",
        "             le = LabelEncoder().fit(CANDIDATE_LABELS)\n",
        "             df_sample['label_text'] = df_sample['label'].astype(str)\n",
        "             try:\n",
        "                 df_sample['label_idx'] = le.transform(df_sample['label_text'])\n",
        "                 print(\"Created 'label_idx' via LabelEncoder based on existing labels.\")\n",
        "             except ValueError:\n",
        "                 print(\"Error: Could not encode labels. Ensure labels match CANDIDATE_LABELS.\")\n",
        "                 df_sample['label_idx'] = -1 # Indicate error\n",
        "        else:\n",
        "             print(\"Error: No usable label column found.\")\n",
        "             sys.exit()\n",
        "\n",
        "\n",
        "    # Fill NaNs in relevant columns\n",
        "    for col in ['speaker', 'subject', 'party_affiliation']:\n",
        "        if col in df_sample.columns:\n",
        "            df_sample[col] = df_sample[col].fillna('Unknown').astype(str)\n",
        "\n",
        "    print(f\"Data loading & prep completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    return df_sample\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 2. Feature Engineering ---\n",
        "# ==============================================================================\n",
        "def create_statement_features(statements):\n",
        "    \"\"\"Generates statement embeddings using SentenceTransformer.\"\"\"\n",
        "    print(\"\\n--- 2a. Generating Statement Features (Embeddings) ---\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        model = SentenceTransformer(SBERT_MODEL)\n",
        "        with torch.no_grad():\n",
        "            # Ensure statements are strings\n",
        "            statement_list = [str(s) if pd.notna(s) else \"\" for s in statements]\n",
        "            embeddings = model.encode(statement_list, convert_to_tensor=True, show_progress_bar=True)\n",
        "        print(f\"Statement embeddings generated with shape: {embeddings.shape}\")\n",
        "        print(f\"Statement feature generation completed in {time.time() - start_time:.2f} seconds.\")\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating statement embeddings: {e}\")\n",
        "        # Determine expected dimension based on model name\n",
        "        sbert_dim = 384 if 'MiniLM' in SBERT_MODEL else (768 if 'base' in SBERT_MODEL else 768) # Default to 768 if unsure\n",
        "        print(f\"Returning fallback zero tensor with dimension {sbert_dim}\")\n",
        "        return torch.zeros((len(statements), sbert_dim))\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 3. Graph Construction ---\n",
        "# ==============================================================================\n",
        "def build_hetero_graph(df, statement_features):\n",
        "    \"\"\"Builds a PyTorch Geometric HeteroData object.\"\"\"\n",
        "    print(\"\\n--- 3. Building Heterogeneous Graph ---\")\n",
        "    start_time = time.time()\n",
        "    data = HeteroData()\n",
        "\n",
        "    # --- Node Mapping and Features ---\n",
        "    # Statements (use DataFrame index as node ID)\n",
        "    statement_node_ids = df.index.to_list()\n",
        "    num_statements = len(statement_node_ids)\n",
        "    data['statement'].x = statement_features\n",
        "    # Map df index to internal graph node index (0 to num_statements-1)\n",
        "    statement_map = {df_idx: graph_idx for graph_idx, df_idx in enumerate(statement_node_ids)}\n",
        "\n",
        "    # Other entities (speaker, subject, party)\n",
        "    entity_maps = {}\n",
        "    entity_embeddings = {}\n",
        "    for node_type in ['speaker', 'subject', 'party']:\n",
        "        col_name = 'party_affiliation' if node_type == 'party' else node_type\n",
        "        if col_name not in df.columns:\n",
        "            print(f\"Warning: Column '{col_name}' not found for node type '{node_type}'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Handle potential multi-value subjects\n",
        "        if node_type == 'subject':\n",
        "            # Ensure splitting happens correctly even if some entries are not strings\n",
        "            all_entities = df[col_name].astype(str).str.split(',').explode().str.strip().unique()\n",
        "        else:\n",
        "            all_entities = df[col_name].unique()\n",
        "\n",
        "        # Filter out potential NaN/None values converted to strings like 'nan'\n",
        "        entity_list = sorted([str(e) for e in all_entities if pd.notna(e) and str(e) and str(e).lower() != 'nan'])\n",
        "        entity_map = {name: i for i, name in enumerate(entity_list)}\n",
        "        entity_maps[node_type] = entity_map\n",
        "        num_entities = len(entity_list)\n",
        "        print(f\"Found {num_entities} unique entities for type '{node_type}'\")\n",
        "\n",
        "        # Initialize learnable embeddings (will be part of the model later)\n",
        "        # data[node_type].x = None # Placeholder, features added in model\n",
        "        data[node_type].num_nodes = num_entities # Store number of nodes\n",
        "\n",
        "    # --- Edge Construction ---\n",
        "    edge_indices = defaultdict(list)\n",
        "\n",
        "    for df_idx, row in df.iterrows():\n",
        "        stmt_graph_idx = statement_map.get(df_idx)\n",
        "        if stmt_graph_idx is None: continue # Should not happen if df.index is used\n",
        "\n",
        "        # Statement -> Speaker\n",
        "        speaker_name = str(row.get('speaker', 'Unknown'))\n",
        "        speaker_map = entity_maps.get('speaker', {})\n",
        "        if speaker_name in speaker_map:\n",
        "            speaker_graph_idx = speaker_map[speaker_name]\n",
        "            edge_indices[('statement', 'spoken_by', 'speaker')].append([stmt_graph_idx, speaker_graph_idx])\n",
        "\n",
        "        # Speaker -> Party\n",
        "        party_name = str(row.get('party_affiliation', 'Unknown'))\n",
        "        party_map = entity_maps.get('party', {})\n",
        "        # Ensure speaker_graph_idx was defined before using it here\n",
        "        if speaker_name in speaker_map and party_name in party_map:\n",
        "            speaker_graph_idx = speaker_map[speaker_name] # Get index again just in case\n",
        "            party_graph_idx = party_map[party_name]\n",
        "            edge_indices[('speaker', 'affiliated_with', 'party')].append([speaker_graph_idx, party_graph_idx])\n",
        "\n",
        "        # Statement -> Subject(s)\n",
        "        subjects_raw = str(row.get('subject', 'Unknown'))\n",
        "        subject_map = entity_maps.get('subject', {})\n",
        "        subjects = [s.strip() for s in subjects_raw.split(',') if s.strip()]\n",
        "        for subject_name in subjects:\n",
        "            if subject_name in subject_map:\n",
        "                subject_graph_idx = subject_map[subject_name]\n",
        "                edge_indices[('statement', 'about_subject', 'subject')].append([stmt_graph_idx, subject_graph_idx])\n",
        "\n",
        "    # Convert edge lists to tensors and add reverse edges\n",
        "    valid_edge_types = [] # Keep track of edge types actually added\n",
        "    for edge_type_tuple in list(edge_indices.keys()):\n",
        "        src_type, rel_type, dst_type = edge_type_tuple\n",
        "        # Ensure list is not empty before converting to tensor\n",
        "        if edge_indices[edge_type_tuple]:\n",
        "            edges = torch.tensor(edge_indices[edge_type_tuple], dtype=torch.long).t().contiguous()\n",
        "            data[src_type, rel_type, dst_type].edge_index = edges\n",
        "            valid_edge_types.append(edge_type_tuple) # Mark this type as valid\n",
        "            # Add reverse edges\n",
        "            rev_rel_type = f\"rev_{rel_type}\"\n",
        "            data[dst_type, rev_rel_type, src_type].edge_index = edges[[1, 0]] # Swap source and destination\n",
        "            valid_edge_types.append((dst_type, rev_rel_type, src_type)) # Mark reverse type as valid\n",
        "        else:\n",
        "             print(f\"Warning: No edges found for type {edge_type_tuple}. Skipping.\")\n",
        "\n",
        "\n",
        "    # Add statement labels and masks\n",
        "    # Ensure label_idx exists and is valid\n",
        "    if 'label_idx' not in df.columns or df['label_idx'].eq(-1).any():\n",
        "        print(\"Error: Invalid 'label_idx' column. Cannot assign labels or masks.\")\n",
        "        # Handle error appropriately, maybe return None or raise exception\n",
        "        return None, None\n",
        "    data['statement'].y = torch.tensor(df['label_idx'].values, dtype=torch.long)\n",
        "\n",
        "    # Create train/test masks for statement nodes\n",
        "    num_all_statements = data['statement'].num_nodes\n",
        "    indices = np.arange(num_all_statements)\n",
        "\n",
        "    # Ensure stratification is possible (at least 2 members per class for splitting)\n",
        "    unique_labels, counts = np.unique(data['statement'].y.numpy(), return_counts=True)\n",
        "    min_samples_per_class = counts.min()\n",
        "    n_splits_required = int(1 / TEST_SPLIT_RATIO) # Approx number of splits for stratify\n",
        "\n",
        "    if min_samples_per_class < n_splits_required and min_samples_per_class < 2:\n",
        "         print(f\"Warning: The least populated class has only {min_samples_per_class} members, which is too few for stratified splitting with test_size={TEST_SPLIT_RATIO}. Performing non-stratified split.\")\n",
        "         stratify_labels = None\n",
        "    else:\n",
        "         stratify_labels = data['statement'].y.numpy()\n",
        "\n",
        "\n",
        "    train_indices, test_indices = train_test_split(\n",
        "        indices,\n",
        "        test_size=TEST_SPLIT_RATIO,\n",
        "        random_state=42,\n",
        "        stratify=stratify_labels # Use labels for stratification if possible\n",
        "        )\n",
        "\n",
        "    train_mask = torch.zeros(num_all_statements, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_all_statements, dtype=torch.bool)\n",
        "    train_mask[train_indices] = True\n",
        "    test_mask[test_indices] = True\n",
        "    data['statement'].train_mask = train_mask\n",
        "    data['statement'].test_mask = test_mask\n",
        "\n",
        "    print(f\"Graph construction completed in {time.time() - start_time:.2f} seconds.\")\n",
        "    print(f\"\\nGraph Summary:\\n{data}\")\n",
        "    # Validate graph structure\n",
        "    try:\n",
        "        data.validate()\n",
        "        print(\"Graph validation successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Graph validation failed: {e}\")\n",
        "\n",
        "    return data, entity_maps, valid_edge_types # Return maps and valid edge types\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 4. GNN Model Definition ---\n",
        "# ==============================================================================\n",
        "class HeteroGNN(torch.nn.Module):\n",
        "    # Pass valid_edge_types to the constructor\n",
        "    def __init__(self, hidden_channels, out_channels, num_layers, entity_maps, embedding_dim, statement_feature_dim, valid_edge_types):\n",
        "        super().__init__()\n",
        "        self.entity_embeddings = torch.nn.ModuleDict()\n",
        "        self.num_entities = {}\n",
        "        self.valid_edge_types = valid_edge_types # Store valid edge types\n",
        "\n",
        "        # Create learnable embeddings for entity types (speaker, subject, party)\n",
        "        for node_type, entity_map in entity_maps.items():\n",
        "            num_entities = len(entity_map)\n",
        "            self.num_entities[node_type] = num_entities\n",
        "            if num_entities > 0:\n",
        "                # Ensure embedding index doesn't go out of bounds if map is empty\n",
        "                self.entity_embeddings[node_type] = Embedding(num_entities, embedding_dim)\n",
        "            else:\n",
        "                print(f\"Warning: No entities found for type {node_type}, embedding layer not created.\")\n",
        "\n",
        "\n",
        "        self.convs = ModuleList()\n",
        "        # Input projection for statement features\n",
        "        self.statement_lin = Linear(statement_feature_dim, hidden_channels)\n",
        "        # Input projection for entity embeddings (only create if embeddings exist)\n",
        "        if self.entity_embeddings:\n",
        "            self.entity_lin = Linear(embedding_dim, hidden_channels)\n",
        "        else:\n",
        "            self.entity_lin = None\n",
        "\n",
        "\n",
        "        in_channels_first = hidden_channels\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            conv_in = in_channels_first if i == 0 else hidden_channels\n",
        "            # *** FIX: Remove add_self_loops argument ***\n",
        "            conv = HeteroConv({\n",
        "                # Only include edge types that actually exist in the graph data\n",
        "                edge_type: SAGEConv((conv_in, conv_in), hidden_channels)\n",
        "                for edge_type in self.valid_edge_types # Use valid edge types here\n",
        "            }, aggr='sum') # Or 'mean', 'max'\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        # Final classifier only on statement nodes\n",
        "        self.classifier = Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        # Apply initial projection and embeddings\n",
        "        feat_dict = {}\n",
        "        # Project statement features\n",
        "        if 'statement' in x_dict and x_dict['statement'] is not None:\n",
        "             # Ensure statement features exist before projecting\n",
        "             if x_dict['statement'].shape[0] > 0:\n",
        "                 feat_dict['statement'] = self.statement_lin(x_dict['statement']).relu()\n",
        "             else:\n",
        "                 # Handle case with no statement nodes if necessary\n",
        "                 feat_dict['statement'] = torch.empty((0, self.statement_lin.out_features), device=self.statement_lin.weight.device)\n",
        "\n",
        "\n",
        "        # Lookup and project entity embeddings\n",
        "        if self.entity_lin: # Check if entity projection layer exists\n",
        "            for node_type, emb_layer in self.entity_embeddings.items():\n",
        "                 # Ensure there are nodes of this type before creating indices\n",
        "                 if self.num_entities[node_type] > 0:\n",
        "                     node_indices = torch.arange(self.num_entities[node_type], device=emb_layer.weight.device)\n",
        "                     node_embeddings = emb_layer(node_indices)\n",
        "                     feat_dict[node_type] = self.entity_lin(node_embeddings).relu()\n",
        "\n",
        "        # GNN layers\n",
        "        # Filter edge_index_dict to only include valid edge types expected by the model\n",
        "        valid_edge_index_dict = {k: v for k, v in edge_index_dict.items() if k in self.valid_edge_types}\n",
        "\n",
        "        for conv in self.convs:\n",
        "            # Pass only the valid edge indices to the convolution\n",
        "            feat_dict = conv(feat_dict, valid_edge_index_dict)\n",
        "            # Apply activation after each layer's aggregation\n",
        "            feat_dict = {key: x.relu() for key, x in feat_dict.items()}\n",
        "\n",
        "        # Classify statement nodes\n",
        "        if 'statement' in feat_dict and feat_dict['statement'].shape[0] > 0:\n",
        "            out = self.classifier(feat_dict['statement'])\n",
        "            return out\n",
        "        else:\n",
        "            # Handle case where statement features might disappear or were never present\n",
        "            print(\"Warning: 'statement' features not found or empty after GNN layers.\")\n",
        "            # Determine expected output shape based on input statement nodes\n",
        "            num_statement_nodes = x_dict.get('statement', torch.empty(0)).shape[0]\n",
        "            return torch.zeros((num_statement_nodes, self.classifier.out_features), device=next(self.parameters()).device)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 5. Training and Evaluation ---\n",
        "# ==============================================================================\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    # Ensure data has features before passing to model\n",
        "    if not hasattr(data, 'x_dict'):\n",
        "         print(\"Error: data object missing 'x_dict'.\")\n",
        "         return 0.0\n",
        "    if not hasattr(data, 'edge_index_dict'):\n",
        "         print(\"Error: data object missing 'edge_index_dict'.\")\n",
        "         return 0.0\n",
        "\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "\n",
        "    # Check if output is valid\n",
        "    if out is None or out.shape[0] == 0:\n",
        "         print(\"Warning: Model output is empty or invalid during training.\")\n",
        "         return 0.0\n",
        "\n",
        "    # Ensure masks and labels exist\n",
        "    if not hasattr(data['statement'], 'train_mask') or not hasattr(data['statement'], 'y'):\n",
        "        print(\"Error: Missing train_mask or labels ('y') on statement nodes.\")\n",
        "        return 0.0\n",
        "\n",
        "    # Get predictions only for training nodes\n",
        "    train_mask = data['statement'].train_mask\n",
        "    if train_mask.sum() == 0:\n",
        "        print(\"Warning: No training samples found in the mask.\")\n",
        "        return 0.0 # Return 0 loss if no samples\n",
        "\n",
        "    # Ensure output tensor shape matches mask length\n",
        "    if out.shape[0] != train_mask.shape[0]:\n",
        "        print(f\"Warning: Output shape {out.shape} mismatch with mask shape {train_mask.shape}. Skipping loss calculation.\")\n",
        "        return 0.0\n",
        "\n",
        "    train_preds = out[train_mask]\n",
        "    train_labels = data['statement'].y[train_mask]\n",
        "\n",
        "    # Check again if filtering resulted in empty tensors\n",
        "    if train_preds.shape[0] == 0:\n",
        "        print(\"Warning: No training samples after applying mask.\")\n",
        "        return 0.0\n",
        "\n",
        "    loss = criterion(train_preds, train_labels)\n",
        "    # Check for NaN loss\n",
        "    if torch.isnan(loss):\n",
        "        print(\"Warning: NaN loss detected during training.\")\n",
        "        return 0.0 # Or handle differently\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, data):\n",
        "    model.eval()\n",
        "    # Ensure data has features before passing to model\n",
        "    if not hasattr(data, 'x_dict') or not hasattr(data, 'edge_index_dict'):\n",
        "         print(\"Error: data object missing 'x_dict' or 'edge_index_dict' for testing.\")\n",
        "         return {}, {} # Return empty results\n",
        "\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "\n",
        "    # Check if output is valid\n",
        "    if out is None or out.shape[0] == 0:\n",
        "         print(\"Warning: Model output is empty or invalid during testing.\")\n",
        "         return {}, {}\n",
        "\n",
        "    # Ensure masks and labels exist\n",
        "    if not hasattr(data['statement'], 'train_mask') or \\\n",
        "       not hasattr(data['statement'], 'test_mask') or \\\n",
        "       not hasattr(data['statement'], 'y'):\n",
        "        print(\"Error: Missing masks or labels ('y') on statement nodes for testing.\")\n",
        "        return {}, {}\n",
        "\n",
        "    # Ensure output tensor shape matches mask length\n",
        "    if out.shape[0] != data['statement'].train_mask.shape[0]:\n",
        "        print(f\"Warning: Output shape {out.shape} mismatch with mask shape {data['statement'].train_mask.shape} during testing.\")\n",
        "        return {}, {}\n",
        "\n",
        "\n",
        "    pred = out.argmax(dim=-1) # Get predicted class index\n",
        "\n",
        "    accs = {}\n",
        "    reports = {}\n",
        "    # Evaluate on train and test masks\n",
        "    for prefix, mask in [('Train', data['statement'].train_mask), ('Test', data['statement'].test_mask)]:\n",
        "         if mask.sum() == 0:\n",
        "             print(f\"Warning: No samples found in {prefix} mask.\")\n",
        "             accs[prefix] = 0.0\n",
        "             reports[prefix] = \"No samples to evaluate.\"\n",
        "             continue\n",
        "\n",
        "         mask_preds = pred[mask].cpu().numpy()\n",
        "         mask_labels = data['statement'].y[mask].cpu().numpy()\n",
        "\n",
        "         # Ensure there are labels to evaluate\n",
        "         if len(mask_labels) == 0:\n",
        "              print(f\"Warning: No labels found for {prefix} mask after filtering.\")\n",
        "              accs[prefix] = 0.0\n",
        "              reports[prefix] = \"No samples to evaluate after filtering.\"\n",
        "              continue\n",
        "\n",
        "         acc = accuracy_score(mask_labels, mask_preds)\n",
        "         # Ensure labels for report generation are correct\n",
        "         report_labels = np.arange(len(CANDIDATE_LABELS)) # Expected labels 0-5\n",
        "         present_labels = np.unique(np.concatenate((mask_labels, mask_preds))) # Actual labels present\n",
        "         # Filter target names to only those present if necessary, or use all expected\n",
        "         target_names = [CANDIDATE_LABELS[i] for i in report_labels if i in present_labels]\n",
        "         # Ensure labels used in report match the unique labels present\n",
        "         report_labels_present = [l for l in report_labels if l in present_labels]\n",
        "\n",
        "\n",
        "         report = classification_report(\n",
        "             mask_labels,\n",
        "             mask_preds,\n",
        "             labels=report_labels_present, # Use labels actually present\n",
        "             target_names=target_names,\n",
        "             zero_division=0\n",
        "         )\n",
        "         accs[prefix] = acc\n",
        "         reports[prefix] = report\n",
        "\n",
        "    return accs, reports\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# --- Main Execution ---\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting LIAR Dataset GNN Classification Pipeline...\")\n",
        "    pipeline_start_time = time.time()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 1. Load and Prepare Data\n",
        "    df = load_and_prep_data(SAMPLE_SIZE)\n",
        "\n",
        "    # Check if labels are valid before proceeding\n",
        "    if 'label_idx' not in df.columns or df['label_idx'].eq(-1).any():\n",
        "         print(\"Exiting due to label errors during data preparation.\")\n",
        "         sys.exit()\n",
        "\n",
        "    # 2. Feature Engineering (Statements)\n",
        "    if 'statement' not in df.columns:\n",
        "        print(\"Error: 'statement' column missing for feature engineering.\")\n",
        "        sys.exit()\n",
        "    statement_features = create_statement_features(df['statement'])\n",
        "    statement_feature_dim = statement_features.shape[1]\n",
        "\n",
        "    # 3. Graph Construction\n",
        "    # Pass valid_edge_types from graph building to model\n",
        "    data, entity_maps, valid_edge_types = build_hetero_graph(df, statement_features)\n",
        "\n",
        "    # Check if graph construction was successful\n",
        "    if data is None:\n",
        "        print(\"Exiting due to errors during graph construction.\")\n",
        "        sys.exit()\n",
        "\n",
        "    data = data.to(device) # Move graph data to the selected device\n",
        "\n",
        "    # 4. Initialize Model\n",
        "    num_classes = len(CANDIDATE_LABELS)\n",
        "    # Pass valid_edge_types to the model constructor\n",
        "    model = HeteroGNN(\n",
        "        hidden_channels=HIDDEN_CHANNELS,\n",
        "        out_channels=num_classes,\n",
        "        num_layers=NUM_GNN_LAYERS,\n",
        "        entity_maps=entity_maps,\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        statement_feature_dim=statement_feature_dim,\n",
        "        valid_edge_types=valid_edge_types # Pass the valid types\n",
        "    ).to(device)\n",
        "    print(\"\\nModel Architecture:\")\n",
        "    print(model)\n",
        "\n",
        "    # 5. Training Loop\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        loss = train(model, data, optimizer, criterion)\n",
        "        # Evaluate periodically\n",
        "        if epoch % 5 == 0 or epoch == 1 or epoch == NUM_EPOCHS:\n",
        "            train_accs, reports = test(model, data)\n",
        "            # Handle potential missing keys if evaluation failed\n",
        "            train_acc_val = train_accs.get('Train', 0.0)\n",
        "            test_acc_val = train_accs.get('Test', 0.0)\n",
        "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
        "                  f'Train Acc: {train_acc_val:.4f}, Test Acc: {test_acc_val:.4f}, '\n",
        "                  f'Time: {time.time() - epoch_start_time:.2f}s')\n",
        "\n",
        "    print(\"--- Training Finished ---\")\n",
        "\n",
        "    # 6. Final Evaluation\n",
        "    print(\"\\n--- Final Evaluation ---\")\n",
        "    final_accs, final_reports = test(model, data)\n",
        "    print(\"\\n--- Test Set Performance ---\")\n",
        "    # Handle potential missing keys if evaluation failed\n",
        "    test_acc_final = final_accs.get('Test', 'N/A')\n",
        "    test_report_final = final_reports.get('Test', 'Evaluation failed.')\n",
        "    if isinstance(test_acc_final, float):\n",
        "        print(f\"Accuracy: {test_acc_final:.4f}\")\n",
        "    else:\n",
        "        print(f\"Accuracy: {test_acc_final}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(test_report_final)\n",
        "\n",
        "\n",
        "    print(f\"\\nPipeline finished in {time.time() - pipeline_start_time:.2f} seconds.\")\n",
        "    print(\"--- End of Script ---\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LIAR Dataset GNN Classification Pipeline...\n",
            "Using device: cpu\n",
            "--- 1. Loading and Preparing Data ---\n",
            "Full dataset shape: (10269, 14)\n",
            "Using a sample of 1000 rows.\n",
            "Created 'label_text' and 'label_idx' columns.\n",
            "Data loading & prep completed in 0.82 seconds.\n",
            "\n",
            "--- 2a. Generating Statement Features (Embeddings) ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52c09b6a9ab2419eaaf34933c4cfbd4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statement embeddings generated with shape: torch.Size([1000, 384])\n",
            "Statement feature generation completed in 17.47 seconds.\n",
            "\n",
            "--- 3. Building Heterogeneous Graph ---\n",
            "Found 557 unique entities for type 'speaker'\n",
            "Found 135 unique entities for type 'subject'\n",
            "Found 14 unique entities for type 'party'\n",
            "Graph construction completed in 0.07 seconds.\n",
            "\n",
            "Graph Summary:\n",
            "HeteroData(\n",
            "  statement={\n",
            "    x=[1000, 384],\n",
            "    y=[1000],\n",
            "    train_mask=[1000],\n",
            "    test_mask=[1000],\n",
            "  },\n",
            "  speaker={ num_nodes=557 },\n",
            "  subject={ num_nodes=135 },\n",
            "  party={ num_nodes=14 },\n",
            "  (statement, spoken_by, speaker)={ edge_index=[2, 1000] },\n",
            "  (speaker, rev_spoken_by, statement)={ edge_index=[2, 1000] },\n",
            "  (speaker, affiliated_with, party)={ edge_index=[2, 1000] },\n",
            "  (party, rev_affiliated_with, speaker)={ edge_index=[2, 1000] },\n",
            "  (statement, about_subject, subject)={ edge_index=[2, 2175] },\n",
            "  (subject, rev_about_subject, statement)={ edge_index=[2, 2175] }\n",
            ")\n",
            "Graph validation successful.\n",
            "\n",
            "Model Architecture:\n",
            "HeteroGNN(\n",
            "  (entity_embeddings): ModuleDict(\n",
            "    (speaker): Embedding(557, 64)\n",
            "    (subject): Embedding(135, 64)\n",
            "    (party): Embedding(14, 64)\n",
            "  )\n",
            "  (convs): ModuleList(\n",
            "    (0-1): 2 x HeteroConv(num_relations=6)\n",
            "  )\n",
            "  (statement_lin): Linear(in_features=384, out_features=128, bias=True)\n",
            "  (entity_lin): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (classifier): Linear(in_features=128, out_features=6, bias=True)\n",
            ")\n",
            "\n",
            "--- Starting Training ---\n",
            "Epoch: 001, Loss: 1.7908, Train Acc: 0.2175, Test Acc: 0.2150, Time: 0.16s\n",
            "Epoch: 005, Loss: 1.7721, Train Acc: 0.2437, Test Acc: 0.1650, Time: 0.12s\n",
            "Epoch: 010, Loss: 1.6049, Train Acc: 0.4300, Test Acc: 0.1500, Time: 0.09s\n",
            "Epoch: 015, Loss: 1.1766, Train Acc: 0.5988, Test Acc: 0.2300, Time: 0.09s\n",
            "Epoch: 020, Loss: 0.7394, Train Acc: 0.7625, Test Acc: 0.2200, Time: 0.09s\n",
            "Epoch: 025, Loss: 0.4014, Train Acc: 0.8700, Test Acc: 0.2250, Time: 0.09s\n",
            "Epoch: 030, Loss: 0.1887, Train Acc: 0.9587, Test Acc: 0.2000, Time: 0.10s\n",
            "Epoch: 035, Loss: 0.0680, Train Acc: 0.9862, Test Acc: 0.2000, Time: 0.09s\n",
            "Epoch: 040, Loss: 0.0204, Train Acc: 0.9975, Test Acc: 0.2200, Time: 0.11s\n",
            "Epoch: 045, Loss: 0.0072, Train Acc: 1.0000, Test Acc: 0.2100, Time: 0.09s\n",
            "Epoch: 050, Loss: 0.0023, Train Acc: 1.0000, Test Acc: 0.2150, Time: 0.09s\n",
            "--- Training Finished ---\n",
            "\n",
            "--- Final Evaluation ---\n",
            "\n",
            "--- Test Set Performance ---\n",
            "Accuracy: 0.2150\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        false       0.36      0.34      0.35        38\n",
            "    half-true       0.16      0.19      0.17        43\n",
            "  mostly-true       0.16      0.17      0.16        36\n",
            "         true       0.04      0.03      0.03        32\n",
            "  barely-true       0.33      0.29      0.31        34\n",
            "pants-on-fire       0.28      0.29      0.29        17\n",
            "\n",
            "     accuracy                           0.21       200\n",
            "    macro avg       0.22      0.22      0.22       200\n",
            " weighted avg       0.22      0.21      0.22       200\n",
            "\n",
            "\n",
            "Pipeline finished in 21.58 seconds.\n",
            "--- End of Script ---\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "52c09b6a9ab2419eaaf34933c4cfbd4b",
            "3579bba4a2a842cc8bf72ce68a3a9697",
            "a9ac3865cbf8424cba55291f4c9c95b7",
            "723d4332e6a04a45966fb53c82f077a3",
            "039bcc61feaf47ca98d278be9fa7de8f",
            "b98db7bcc4b54a64be9a17a14de0b56e",
            "5f83015687ee494d8898d908e76ffd71",
            "13902f60e0b1413687f771e62562aeaa",
            "6ba1502c5d6245e0bc18a9bcf534785f",
            "f59e740f460f44f8a8379dc7840cd718",
            "b6f9898f993840528648b45908ae2892"
          ]
        },
        "id": "fzG-xHRc6qQw",
        "outputId": "33579d2c-5a31-4ce5-acf1-8aac9e7c6eac"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7y-g1cqM61Lx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "52c09b6a9ab2419eaaf34933c4cfbd4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3579bba4a2a842cc8bf72ce68a3a9697",
              "IPY_MODEL_a9ac3865cbf8424cba55291f4c9c95b7",
              "IPY_MODEL_723d4332e6a04a45966fb53c82f077a3"
            ],
            "layout": "IPY_MODEL_039bcc61feaf47ca98d278be9fa7de8f"
          }
        },
        "3579bba4a2a842cc8bf72ce68a3a9697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b98db7bcc4b54a64be9a17a14de0b56e",
            "placeholder": "​",
            "style": "IPY_MODEL_5f83015687ee494d8898d908e76ffd71",
            "value": "Batches: 100%"
          }
        },
        "a9ac3865cbf8424cba55291f4c9c95b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13902f60e0b1413687f771e62562aeaa",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ba1502c5d6245e0bc18a9bcf534785f",
            "value": 32
          }
        },
        "723d4332e6a04a45966fb53c82f077a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f59e740f460f44f8a8379dc7840cd718",
            "placeholder": "​",
            "style": "IPY_MODEL_b6f9898f993840528648b45908ae2892",
            "value": " 32/32 [00:16&lt;00:00,  2.87it/s]"
          }
        },
        "039bcc61feaf47ca98d278be9fa7de8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b98db7bcc4b54a64be9a17a14de0b56e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f83015687ee494d8898d908e76ffd71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13902f60e0b1413687f771e62562aeaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba1502c5d6245e0bc18a9bcf534785f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f59e740f460f44f8a8379dc7840cd718": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6f9898f993840528648b45908ae2892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}